{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The purpose is to create a classifier to classify the articles into \"flood\" and \"not-flood\"\n",
    "Using the title of the article.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one of the .csv file prepared from article scraping task # 4\n",
    "df = pd.read_csv('FL-2019-000022-IRN.csv', index_col = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on what @Luca Pietrobon prepared:\n",
    "\n",
    "df = df.dropna(axis = 1, how = 'all').rename(columns={c: 'Categories' for c in ['category', 'Category']})\n",
    "\n",
    "df = df.drop(columns = [c for c in df.columns if c.startswith('Unnamed')]).rename(columns = {c: c.lower().replace (' ', '_') for c in df.columns})\n",
    "\n",
    "df['label_isFlood'] = 1 *df.categories.str.contains('flood')\n",
    "df['text'] = df.article_title.str.split(r'\\s-|\\|\\s').str[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the DF\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "#Split the DF\n",
    "y = df.pop('label_isFlood')\n",
    "X = df\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 42)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SnowballStemmer to perform stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Define a function to perform both stemming and tokenization\n",
    "def tokenize_and_stem(text):\n",
    "    \n",
    "    # Tokenize by sentence, then by word\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    # Filter out raw tokens to remove noise\n",
    "    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "    \n",
    "    \n",
    "    # Stem the filtered_tokens\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    \n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer to create TF-IDF vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate TfidfVectorizer object with stopwords and tokenizer\n",
    "# parameters for efficient processing of text\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=5,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem,\n",
    "                                 ngram_range=(1,3))\n",
    "\n",
    "    \n",
    "# Fit and transform the tfidf_vectorizer with the \"Title (text column in DF)\" \n",
    "# to create a vector representation \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in X_train['text']])\n",
    "\n",
    "tfidf_matrix2 = tfidf_vectorizer.fit_transform([x for x in X_test['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Naive Bayes to do the prediction\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(tfidf_matrix.toarray(),y_train)\n",
    "\n",
    "y_hat = (clf.predict(tfidf_matrix2.toarray()))\n",
    "\n",
    "clf.score(tfidf_matrix.toarray(),y_train)\n",
    "clf.score(tfidf_matrix2.toarray(),y_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
