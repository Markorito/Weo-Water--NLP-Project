{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sv1kxP_-uAI",
    "outputId": "af8d6862-e2e1-4822-c4cd-d03a16c6f872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygooglenews\n",
      "  Downloading https://files.pythonhosted.org/packages/59/cb/84f162888a07e501630f6be7c70997c5c8afd5ba4a40c3a079c321e97c29/pygooglenews-0.1.2-py3-none-any.whl\n",
      "Collecting beautifulsoup4<5.0.0,>=4.9.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 6.5MB/s \n",
      "\u001b[?25hCollecting requests<3.0.0,>=2.24.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 4.7MB/s \n",
      "\u001b[?25hCollecting feedparser<6.0.0,>=5.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 8.7MB/s \n",
      "\u001b[?25hCollecting dateparser<0.8.0,>=0.7.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/d5/5a2e51bc0058f66b54669735f739d27afc3eb453ab00520623c7ab168e22/dateparser-0.7.6-py2.py3-none-any.whl (362kB)\n",
      "\u001b[K     |████████████████████████████████| 368kB 9.0MB/s \n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\"\n",
      "  Downloading https://files.pythonhosted.org/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->pygooglenews) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->pygooglenews) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->pygooglenews) (1.24.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->pygooglenews) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser<0.8.0,>=0.7.6->pygooglenews) (2.8.1)\n",
      "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.7/dist-packages (from dateparser<0.8.0,>=0.7.6->pygooglenews) (2019.12.20)\n",
      "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser<0.8.0,>=0.7.6->pygooglenews) (1.5.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser<0.8.0,>=0.7.6->pygooglenews) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->dateparser<0.8.0,>=0.7.6->pygooglenews) (1.15.0)\n",
      "Building wheels for collected packages: feedparser\n",
      "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for feedparser: filename=feedparser-5.2.1-cp37-none-any.whl size=44940 sha256=8d015894c595378fb7164abf4ee64b8c0fee93b42aa6071fd9a1b473667d7f70\n",
      "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
      "Successfully built feedparser\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: soupsieve, beautifulsoup4, requests, feedparser, dateparser, pygooglenews\n",
      "  Found existing installation: beautifulsoup4 4.6.3\n",
      "    Uninstalling beautifulsoup4-4.6.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.6.3\n",
      "  Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "Successfully installed beautifulsoup4-4.9.3 dateparser-0.7.6 feedparser-5.2.1 pygooglenews-0.1.2 requests-2.25.1 soupsieve-2.2.1\n",
      "Collecting newspaper3k\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 5.2MB/s \n",
      "\u001b[?25hCollecting tldextract>=2.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 7.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
      "Collecting tinysegmenter==0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
      "Collecting cssselect>=0.9.2\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
      "Collecting feedfinder2>=0.0.4\n",
      "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (5.2.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.25.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.1)\n",
      "Collecting jieba3k>=0.35.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4MB 10.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.9.3)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.10)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.2.1)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp37-none-any.whl size=13538 sha256=d75ad7ce46b39caf6219f976ad667ffea3900401567f69242cfd01fd5ccfd303\n",
      "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp37-none-any.whl size=3358 sha256=c8f3b61d68cb575888261e612c7b8004ae74ac643b623b36869ae8f21e7758d5\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp37-none-any.whl size=7398406 sha256=639a62519f37ab0e0e3a6b3a78844967e75563b79c0666399517d5b07ed13175\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k\n",
      "Installing collected packages: requests-file, tldextract, tinysegmenter, cssselect, feedfinder2, jieba3k, newspaper3k\n",
      "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 tinysegmenter-0.3 tldextract-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pygooglenews\n",
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XAOzHuKP-smf"
   },
   "outputs": [],
   "source": [
    "from pygooglenews import GoogleNews\n",
    "from newspaper import Article\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UzBdZp8D-9fI"
   },
   "outputs": [],
   "source": [
    "language = 'en'\n",
    "country = 'Uganda'\n",
    "\n",
    "search_terms = 'Flood Uganda'\n",
    "start_date = '2016-01-01'\n",
    "end_date = '2021-05-23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "t7AUt6OD_Kpm"
   },
   "outputs": [],
   "source": [
    "gn = GoogleNews(lang = language, country = country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cnr5OY9d_lqr"
   },
   "outputs": [],
   "source": [
    "# function to get the article content and other details from article url\n",
    "def get_article_content(article_url):\n",
    "    # get the date of posting the article\n",
    "    try:\n",
    "        news_article = Article(article_url,language='en')\n",
    "        news_article.download()\n",
    "        news_article.parse()\n",
    "        news_article.nlp()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # get the author\n",
    "    #print(news_article.authors)\n",
    "\n",
    "    # get the publish date\n",
    "    #print(news_article.publish_date)\n",
    "\n",
    "    #get top image\n",
    "    #print(news_article.top_image)\n",
    "\n",
    "    #get a summary of article\n",
    "    #print(news_article.summary)\n",
    "\n",
    "    # get article keywords\n",
    "    #print(news_article.keywords)\n",
    "\n",
    "    #get the article text\n",
    "    return news_article.text,news_article.top_image,news_article.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hioI4hsY_mtP"
   },
   "outputs": [],
   "source": [
    "# function to get top 50 new links\n",
    "def get_news_links(search_terms,start_date,end_date,event_id):\n",
    "    article_num =[]\n",
    "    article_title = []\n",
    "    article_link = []\n",
    "    article_content = []\n",
    "    publishing_date = []\n",
    "    article_image = []\n",
    "    article_keywords =[]\n",
    "    article_relevance = []\n",
    "    search = gn.search(query=search_terms, helper = True, when = None, from_ = start_date, to_ = end_date , proxies=None, scraping_bee=None)\n",
    "    count = 0\n",
    "    for item in search['entries']:\n",
    "        print(count)\n",
    "        article_num.append(count)\n",
    "        count += 1\n",
    "        article_title.append(item['title'])\n",
    "        article_link.append(item['link'])\n",
    "        article_content.append(get_article_content(item['link'])[0])\n",
    "        publishing_date.append(item['published'])\n",
    "        article_image.append(get_article_content(item['link'])[1])\n",
    "        article_keywords.append(get_article_content(item['link'])[2])\n",
    "        \n",
    "    event_id_list = [event_id]*count\n",
    "    article_relevance_list = [' ']*count\n",
    "    articles_dict = {'ID': article_num,'event_id': event_id_list,'article_title':article_title,'article_link':article_link,'article_content':article_content,'publishing_date':publishing_date,'article_image':article_image,'article_keywords':article_keywords,'article_relevance':article_relevance_list}\n",
    "    print(len(article_num))\n",
    "    print(len(article_title))\n",
    "    print(len(article_link))\n",
    "    print(len(article_content))\n",
    "    print(len(publishing_date))\n",
    "    print(len(article_image))\n",
    "    print(len(article_keywords))\n",
    "    print(len(article_relevance))\n",
    "    df = pd.DataFrame(articles_dict)\n",
    "    #news_results_list = zip(article_num,article_title,article_link,article_content,publishing_date,article_image,article_keywords,article_relevance)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0pe7kHTWz5a",
    "outputId": "f26cf8b5-4b84-41c0-bbfd-7f19d23c571a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# searching for disaster events in news articles\n",
    "# event_id is the glide id for which we are scraping the news_articles\n",
    "\n",
    "ids_list = ['FL-2011-000132-UGA', 'FL-2020-000132-UGA', 'FL-2019-000163-UGA', \n",
    "            'FL-2019-000066-UGA']\n",
    "\n",
    "news_results_list = []\n",
    "for item in ids_list:\n",
    "    news_results_list.append(get_news_links(search_terms,start_date,end_date, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vGTc9O40ZeGk"
   },
   "outputs": [],
   "source": [
    "df_news_results = pd.concat(news_results_list)\n",
    "\n",
    "df_news_results.to_csv('FL-Uganda-EN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898
    },
    "id": "lHaALo2EbNtC",
    "outputId": "8af6f2f7-bf1c-4a8f-d614-d121dc3294c1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>event_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_link</th>\n",
       "      <th>article_content</th>\n",
       "      <th>publishing_date</th>\n",
       "      <th>article_image</th>\n",
       "      <th>article_keywords</th>\n",
       "      <th>article_relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FL-2011-000132-UGA</td>\n",
       "      <td>Uganda Food Security Outlook Update April 2021...</td>\n",
       "      <td>https://reliefweb.int/report/burundi/uganda-fo...</td>\n",
       "      <td>KEY MESSAGES\\n\\n• Delayed and below-average cu...</td>\n",
       "      <td>Mon, 03 May 2021 07:00:00 GMT</td>\n",
       "      <td>https://reliefweb.int/sites/reliefweb.int/file...</td>\n",
       "      <td>[delayed, outlook, update, security, rainfall,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FL-2011-000132-UGA</td>\n",
       "      <td>Uganda: Joy As Flood-Affected Schools Get New ...</td>\n",
       "      <td>https://allafrica.com/stories/202105170316.html</td>\n",
       "      <td>By Joel Kaguta\\n\\nPupils in the Rwenzori Sub-r...</td>\n",
       "      <td>Sat, 15 May 2021 13:30:00 GMT</td>\n",
       "      <td>https://cdn08.allafrica.com/static/images/stru...</td>\n",
       "      <td>[primary, classrooms, floodaffected, school, c...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>FL-2011-000132-UGA</td>\n",
       "      <td>Relocate to safer areas, govt tells flood vict...</td>\n",
       "      <td>https://www.monitor.co.ug/uganda/news/national...</td>\n",
       "      <td>By Wilson Kutamba More by this Author\\n\\nGover...</td>\n",
       "      <td>Fri, 14 May 2021 11:00:14 GMT</td>\n",
       "      <td>https://www.monitor.co.ug/resource/blob/340014...</td>\n",
       "      <td>[rising, relocate, safer, lake, water, flood, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>FL-2011-000132-UGA</td>\n",
       "      <td>Poor human activities aiding Butaleja floods -...</td>\n",
       "      <td>https://www.monitor.co.ug/uganda/news/national...</td>\n",
       "      <td>By Yahudu Kitunzi More by this Author\\n\\nEnvir...</td>\n",
       "      <td>Mon, 10 May 2021 07:00:00 GMT</td>\n",
       "      <td>https://www.monitor.co.ug/resource/blob/339440...</td>\n",
       "      <td>[aiding, poor, water, activities, environmenta...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>FL-2011-000132-UGA</td>\n",
       "      <td>Battling Rising Water in Uganda's Wetlands - G...</td>\n",
       "      <td>https://globalpressjournal.com/africa/uganda/b...</td>\n",
       "      <td>KIKUUBE, UGANDA — Standing on the shore of Lak...</td>\n",
       "      <td>Sun, 21 Feb 2021 08:00:00 GMT</td>\n",
       "      <td>https://globalpressjournal.com/wp-content/uplo...</td>\n",
       "      <td>[rising, battling, water, lake, sanyu, ugandas...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>FL-2019-000066-UGA</td>\n",
       "      <td>Government rush to rescue flood victims on Lak...</td>\n",
       "      <td>https://www.ntv.co.ug/ug/news/national/governm...</td>\n",
       "      <td>Government rush to rescue flood victims on Lak...</td>\n",
       "      <td>Sat, 24 Oct 2020 07:00:00 GMT</td>\n",
       "      <td>https://i.ytimg.com/vi/GheVqXZ_kyU/mqdefault.jpg</td>\n",
       "      <td>[hoima, tonya, lake, flood, landing, rush, bun...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>FL-2019-000066-UGA</td>\n",
       "      <td>Flood Victims in Kwania District Demand For Re...</td>\n",
       "      <td>https://ugandaradionetwork.net/story/flood-vic...</td>\n",
       "      <td>The flash floods have so far displaced residen...</td>\n",
       "      <td>Fri, 05 Jun 2020 07:00:00 GMT</td>\n",
       "      <td>https://ugandaradionetwork.net/a/helpers/image...</td>\n",
       "      <td>[lake, water, flood, sub, temporal, relief, ra...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>FL-2019-000066-UGA</td>\n",
       "      <td>Floods, landslides affect thousands of people ...</td>\n",
       "      <td>http://www.xinhuanet.com/english/2020-05/03/c_...</td>\n",
       "      <td>Source: Xinhua| 2020-05-03 23:40:37|Editor: hu...</td>\n",
       "      <td>Sun, 03 May 2020 07:00:00 GMT</td>\n",
       "      <td></td>\n",
       "      <td>[rains, season, rainfall, western, landslides,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>FL-2019-000066-UGA</td>\n",
       "      <td>Vanessa Nakate: 'Many people are not yet aware...</td>\n",
       "      <td>https://www.dw.com/en/fridays-for-future-ugand...</td>\n",
       "      <td>From Uganda and in her early 20s, Vanessa Naka...</td>\n",
       "      <td>Wed, 02 Sep 2020 07:00:00 GMT</td>\n",
       "      <td>https://static.dw.com/image/52445320_6.jpeg</td>\n",
       "      <td>[vanessa, climate, thing, different, water, ch...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>FL-2019-000066-UGA</td>\n",
       "      <td>2020 Rewind: Five Major Disasters That Wreaked...</td>\n",
       "      <td>https://weather.com/en-IN/india/news/news/2020...</td>\n",
       "      <td>Cyclone Amphan induces high, rough tides at Ka...</td>\n",
       "      <td>Thu, 31 Dec 2020 08:00:00 GMT</td>\n",
       "      <td>https://s.w-x.co/in-locust_swarm_0.jpg</td>\n",
       "      <td>[havoc, damage, pandemic, affected, covid19, i...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  ... article_relevance\n",
       "0    0  ...                  \n",
       "1    1  ...                  \n",
       "2    2  ...                  \n",
       "3    3  ...                  \n",
       "4    4  ...                  \n",
       "..  ..  ...               ...\n",
       "95  95  ...                  \n",
       "96  96  ...                  \n",
       "97  97  ...                  \n",
       "98  98  ...                  \n",
       "99  99  ...                  \n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "News Scraping - Uganda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
